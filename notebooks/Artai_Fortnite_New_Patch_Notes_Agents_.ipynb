{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pugawooga/TeamArtai-Fortnite-Patch-Agents/blob/main/Artai_Fortnite_New_Patch_Notes_Agents_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gsp6p00-Ii68"
      },
      "source": [
        "## **Installing Python libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSz4JxlaHhTs",
        "outputId": "96efcfc7-3a4a-45b4-aba2-65191ce0502a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-1.1.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting langchain-azure-ai\n",
            "  Downloading langchain_azure_ai-1.0.3-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-1.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.13.0-cp312-cp312-win_amd64.whl.metadata (7.7 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting langchain-core<2.0.0,>=1.1.0 (from langchain)\n",
            "  Downloading langchain_core-1.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting langgraph<1.1.0,>=1.0.2 (from langchain)\n",
            "  Downloading langgraph-1.0.4-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from langchain) (2.8.2)\n",
            "Collecting openai<3.0.0,>=1.109.1 (from langchain-openai)\n",
            "  Downloading openai-2.8.1-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting tiktoken<1.0.0,>=0.7.0 (from langchain-openai)\n",
            "  Downloading tiktoken-0.12.0-cp312-cp312-win_amd64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: aiohttp<4.0,>=3.10 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from langchain-azure-ai) (3.10.5)\n",
            "Collecting azure-ai-agents<1.3.0,>=1.2.0b3 (from langchain-azure-ai)\n",
            "  Downloading azure_ai_agents-1.2.0b6-py3-none-any.whl.metadata (74 kB)\n",
            "Collecting azure-ai-inference<2.0,>=1.0.0b9 (from azure-ai-inference[opentelemetry]<2.0,>=1.0.0b9->langchain-azure-ai)\n",
            "  Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting azure-ai-projects<2.0,>=1.0 (from langchain-azure-ai)\n",
            "  Downloading azure_ai_projects-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting azure-core<2.0,>=1.32 (from langchain-azure-ai)\n",
            "  Downloading azure_core-1.36.0-py3-none-any.whl.metadata (47 kB)\n",
            "Collecting azure-cosmos<5.0,>=4.14.0b1 (from langchain-azure-ai)\n",
            "  Downloading azure_cosmos-4.14.2-py3-none-any.whl.metadata (105 kB)\n",
            "Collecting azure-identity<2.0,>=1.15 (from langchain-azure-ai)\n",
            "  Downloading azure_identity-1.25.1-py3-none-any.whl.metadata (88 kB)\n",
            "Collecting azure-search-documents<12.0,>=11.4 (from langchain-azure-ai)\n",
            "  Downloading azure_search_documents-11.6.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from langchain-azure-ai) (1.26.4)\n",
            "Collecting six<2.0.0,>=1.17.0 (from langchain-azure-ai)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting langchain-classic<2.0.0,>=1.0.0 (from langchain-community)\n",
            "  Downloading langchain_classic-1.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from langchain-community) (2.0.34)\n",
            "Collecting requests<3.0.0,>=2.32.5 (from langchain-community)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from langchain-community) (8.2.3)\n",
            "Collecting dataclasses-json<0.7.0,>=0.6.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.10.1 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting langsmith<1.0.0,>=0.1.125 (from langchain-community)\n",
            "  Downloading langsmith-0.4.49-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.3-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: packaging in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from python-docx) (4.11.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from aiohttp<4.0,>=3.10->langchain-azure-ai) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from aiohttp<4.0,>=3.10->langchain-azure-ai) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from aiohttp<4.0,>=3.10->langchain-azure-ai) (23.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from aiohttp<4.0,>=3.10->langchain-azure-ai) (1.4.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from aiohttp<4.0,>=3.10->langchain-azure-ai) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from aiohttp<4.0,>=3.10->langchain-azure-ai) (1.11.0)\n",
            "Collecting isodate>=0.6.1 (from azure-ai-agents<1.3.0,>=1.2.0b3->langchain-azure-ai)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting azure-core-tracing-opentelemetry (from azure-ai-inference[opentelemetry]<2.0,>=1.0.0b9->langchain-azure-ai)\n",
            "  Downloading azure_core_tracing_opentelemetry-1.0.0b12-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting typing_extensions>=4.9.0 (from python-docx)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting azure-storage-blob>=12.15.0 (from azure-ai-projects<2.0,>=1.0->langchain-azure-ai)\n",
            "  Downloading azure_storage_blob-12.27.1-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: cryptography>=2.5 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from azure-identity<2.0,>=1.15->langchain-azure-ai) (43.0.0)\n",
            "Collecting msal>=1.30.0 (from azure-identity<2.0,>=1.15->langchain-azure-ai)\n",
            "  Downloading msal-1.34.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting msal-extensions>=1.2.0 (from azure-identity<2.0,>=1.15->langchain-azure-ai)\n",
            "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting azure-common>=1.1 (from azure-search-documents<12.0,>=11.4->langchain-azure-ai)\n",
            "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7.0,>=0.6.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from langchain-core<2.0.0,>=1.1.0->langchain) (1.33)\n",
            "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl.metadata (5.2 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading langgraph_sdk-0.2.10-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.27.0)\n",
            "Collecting orjson>=3.9.14 (from langsmith<1.0.0,>=0.1.125->langchain-community)\n",
            "  Downloading orjson-3.11.4-cp312-cp312-win_amd64.whl.metadata (42 kB)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.2.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Collecting jiter<1,>=0.10.0 (from openai<3.0.0,>=1.109.1->langchain-openai)\n",
            "  Downloading jiter-0.12.0-cp312-cp312-win_amd64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: sniffio in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.20.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.21.0)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic-settings<3.0.0,>=2.10.1->langchain-community)\n",
            "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2024.12.14)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.0.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: cffi>=1.12 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from cryptography>=2.5->azure-identity<2.0,>=1.15->langchain-azure-ai) (1.17.1)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.1.0->langchain) (2.1)\n",
            "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain)\n",
            "  Downloading ormsgpack-1.12.0-cp312-cp312-win_amd64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: PyJWT<3,>=1.0.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity<2.0,>=1.15->langchain-azure-ai) (2.8.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from tqdm>4->openai<3.0.0,>=1.109.1->langchain-openai) (0.4.6)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.0.0)\n",
            "Collecting opentelemetry-api>=1.12.0 (from azure-core-tracing-opentelemetry->azure-ai-inference[opentelemetry]<2.0,>=1.0.0b9->langchain-azure-ai)\n",
            "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pycparser in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity<2.0,>=1.15->langchain-azure-ai) (2.21)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from opentelemetry-api>=1.12.0->azure-core-tracing-opentelemetry->azure-ai-inference[opentelemetry]<2.0,>=1.0.0b9->langchain-azure-ai) (7.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in c:\\users\\nthie\\anaconda3\\lib\\site-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.12.0->azure-core-tracing-opentelemetry->azure-ai-inference[opentelemetry]<2.0,>=1.0.0b9->langchain-azure-ai) (3.17.0)\n",
            "Downloading langchain-1.1.0-py3-none-any.whl (101 kB)\n",
            "Downloading langchain_openai-1.1.0-py3-none-any.whl (84 kB)\n",
            "Downloading langchain_azure_ai-1.0.3-py3-none-any.whl (99 kB)\n",
            "Downloading langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
            "   ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
            "   ---- ----------------------------------- 0.3/2.5 MB ? eta -:--:--\n",
            "   ------------ --------------------------- 0.8/2.5 MB 2.8 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 1.6/2.5 MB 3.0 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 2.4/2.5 MB 3.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.5/2.5 MB 3.1 MB/s eta 0:00:00\n",
            "Downloading langchain_text_splitters-1.0.0-py3-none-any.whl (33 kB)\n",
            "Downloading faiss_cpu-1.13.0-cp312-cp312-win_amd64.whl (18.7 MB)\n",
            "   ---------------------------------------- 0.0/18.7 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.8/18.7 MB 4.8 MB/s eta 0:00:04\n",
            "   ---- ----------------------------------- 2.1/18.7 MB 4.9 MB/s eta 0:00:04\n",
            "   ------- -------------------------------- 3.4/18.7 MB 5.8 MB/s eta 0:00:03\n",
            "   ---------- ----------------------------- 4.7/18.7 MB 5.9 MB/s eta 0:00:03\n",
            "   ------------- -------------------------- 6.3/18.7 MB 6.3 MB/s eta 0:00:02\n",
            "   ---------------- ----------------------- 7.9/18.7 MB 6.6 MB/s eta 0:00:02\n",
            "   --------------------- ------------------ 10.0/18.7 MB 6.9 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 12.1/18.7 MB 7.3 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 14.4/18.7 MB 7.9 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 16.8/18.7 MB 8.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  18.6/18.7 MB 8.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 18.7/18.7 MB 7.6 MB/s eta 0:00:00\n",
            "Downloading pypdf-6.4.0-py3-none-any.whl (329 kB)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "Downloading azure_ai_agents-1.2.0b6-py3-none-any.whl (217 kB)\n",
            "Downloading azure_ai_inference-1.0.0b9-py3-none-any.whl (124 kB)\n",
            "Downloading azure_ai_projects-1.0.0-py3-none-any.whl (115 kB)\n",
            "Downloading azure_core-1.36.0-py3-none-any.whl (213 kB)\n",
            "Downloading azure_cosmos-4.14.2-py3-none-any.whl (388 kB)\n",
            "Downloading azure_identity-1.25.1-py3-none-any.whl (191 kB)\n",
            "Downloading azure_search_documents-11.6.0-py3-none-any.whl (307 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.3-py3-none-any.whl (9.0 kB)\n",
            "Downloading langchain_classic-1.0.0-py3-none-any.whl (1.0 MB)\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.0/1.0 MB 8.3 MB/s eta 0:00:00\n",
            "Downloading langchain_core-1.1.0-py3-none-any.whl (473 kB)\n",
            "Downloading langgraph-1.0.4-py3-none-any.whl (157 kB)\n",
            "Downloading langsmith-0.4.49-py3-none-any.whl (410 kB)\n",
            "Downloading openai-2.8.1-py3-none-any.whl (1.0 MB)\n",
            "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.0/1.0 MB 9.6 MB/s eta 0:00:00\n",
            "Downloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading tiktoken-0.12.0-cp312-cp312-win_amd64.whl (878 kB)\n",
            "   ---------------------------------------- 0.0/878.7 kB ? eta -:--:--\n",
            "   ---------------------------------------- 878.7/878.7 kB 9.8 MB/s eta 0:00:00\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
            "Downloading azure_storage_blob-12.27.1-py3-none-any.whl (428 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading jiter-0.12.0-cp312-cp312-win_amd64.whl (205 kB)\n",
            "Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
            "Downloading langgraph_prebuilt-1.0.5-py3-none-any.whl (35 kB)\n",
            "Downloading langgraph_sdk-0.2.10-py3-none-any.whl (58 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading msal-1.34.0-py3-none-any.whl (116 kB)\n",
            "Downloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
            "Downloading orjson-3.11.4-cp312-cp312-win_amd64.whl (131 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading xxhash-3.6.0-cp312-cp312-win_amd64.whl (31 kB)\n",
            "Downloading azure_core_tracing_opentelemetry-1.0.0b12-py3-none-any.whl (11 kB)\n",
            "Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
            "Downloading ormsgpack-1.12.0-cp312-cp312-win_amd64.whl (112 kB)\n",
            "Installing collected packages: azure-common, xxhash, typing_extensions, six, requests, pypdf, ormsgpack, orjson, marshmallow, jiter, isodate, httpx-sse, faiss-cpu, typing-inspection, typing-inspect, tiktoken, python-docx, opentelemetry-api, azure-core, langgraph-sdk, dataclasses-json, azure-storage-blob, azure-search-documents, azure-cosmos, azure-core-tracing-opentelemetry, azure-ai-inference, azure-ai-agents, pydantic-settings, openai, msal, langsmith, azure-ai-projects, msal-extensions, langchain-core, langgraph-checkpoint, langchain-text-splitters, langchain-openai, azure-identity, langgraph-prebuilt, langchain-classic, langgraph, langchain-community, langchain, langchain-azure-ai\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.11.0\n",
            "    Uninstalling typing_extensions-4.11.0:\n",
            "      Successfully uninstalled typing_extensions-4.11.0\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pydantic-settings\n",
            "    Found existing installation: pydantic-settings 2.6.1\n",
            "    Uninstalling pydantic-settings-2.6.1:\n",
            "      Successfully uninstalled pydantic-settings-2.6.1\n",
            "Successfully installed azure-ai-agents-1.2.0b6 azure-ai-inference-1.0.0b9 azure-ai-projects-1.0.0 azure-common-1.1.28 azure-core-1.36.0 azure-core-tracing-opentelemetry-1.0.0b12 azure-cosmos-4.14.2 azure-identity-1.25.1 azure-search-documents-11.6.0 azure-storage-blob-12.27.1 dataclasses-json-0.6.7 faiss-cpu-1.13.0 httpx-sse-0.4.3 isodate-0.7.2 jiter-0.12.0 langchain-1.1.0 langchain-azure-ai-1.0.3 langchain-classic-1.0.0 langchain-community-0.4.1 langchain-core-1.1.0 langchain-openai-1.1.0 langchain-text-splitters-1.0.0 langgraph-1.0.4 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.5 langgraph-sdk-0.2.10 langsmith-0.4.49 marshmallow-3.26.1 msal-1.34.0 msal-extensions-1.3.1 openai-2.8.1 opentelemetry-api-1.38.0 orjson-3.11.4 ormsgpack-1.12.0 pydantic-settings-2.12.0 pypdf-6.4.0 python-docx-1.2.0 requests-2.32.5 six-1.17.0 tiktoken-0.12.0 typing-inspect-0.9.0 typing-inspection-0.4.2 typing_extensions-4.15.0 xxhash-3.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain langchain-openai langchain-azure-ai langchain-community langchain-text-splitters faiss-cpu pypdf python-docx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZXYzjyZIGMJ"
      },
      "source": [
        "## **Azure API**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "M6BVFpAxIrsI"
      },
      "outputs": [],
      "source": [
        "\n",
        "AZURE_API_KEY = \"b322ba1b1ee64ddcaf10d4b8c3c97d58\"\n",
        "CLASS = \"MIS372T\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "owVgruz4JGzC"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from typing import List\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Tuple\n",
        "\n",
        "# APIM bases\n",
        "AZURE_INFERENCE_BASE = f\"https://aistudio-apim-ai-gateway02.azure-api.net/{CLASS}/v1/models\"  # chat base\n",
        "AZURE_OPENAI_BASE    = f\"https://aistudio-apim-ai-gateway02.azure-api.net/{CLASS}/v1\"         # embeddings base\n",
        "\n",
        "\n",
        "# - For Azure AI Inference chat, `credential` is the API key forwarded to your backend (or validated at APIM).\n",
        "# - For Azure OpenAI embeddings, `openai_api_key` is forwarded to your backend.\n",
        "# - If APIM requires an additional subscription header, set APIM_SUBSCRIPTION_KEY.\n",
        "AZURE_INFERENCE_API_KEY = os.getenv(\"AZURE_INFERENCE_API_KEY\", \"LEAVE_ALONE\")\n",
        "AZURE_OPENAI_API_KEY    = os.getenv(\"AZURE_OPENAI_API_KEY\", \"LEAVE_ALONE\")\n",
        "APIM_SUBSCRIPTION_KEY   = AZURE_API_KEY\n",
        "\n",
        "# Versions / names\n",
        "CHAT_API_VERSION = \"2024-05-01-preview\"\n",
        "EMBED_API_VERSION = \"2023-05-15\"\n",
        "CHAT_MODEL_NAME = \"gpt-4.1-nano\"         # The model name you exposed via APIM for chat\n",
        "EMBED_DEPLOYMENT = \"text-embedding-3-small\"  # The Azure OpenAI embedding deployment name behind APIM\n",
        "\n",
        "# Headers for APIM (optional, depending on your policy).\n",
        "APIM_HEADERS = {\"Ocp-Apim-Subscription-Key\": APIM_SUBSCRIPTION_KEY} if APIM_SUBSCRIPTION_KEY else {}\n",
        "\n",
        "## Define the load_text_from_file function to load text from files, supporting TXT, PDF, DOCX\n",
        "def load_text_from_file(path: str) -> str:\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f\"File not found: {path}\")\n",
        "    suffix = p.suffix.lower()\n",
        "\n",
        "    if suffix in [\".txt\", \".md\"]:\n",
        "        return p.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "\n",
        "    if suffix == \".pdf\":\n",
        "        try:\n",
        "            from pypdf import PdfReader\n",
        "        except Exception:\n",
        "            raise RuntimeError(\"pypdf is required for PDFs. Install via `pip install pypdf`.\")\n",
        "        text_parts = []\n",
        "        reader = PdfReader(str(p))\n",
        "        for page in reader.pages:\n",
        "            t = page.extract_text() or \"\"\n",
        "            text_parts.append(t)\n",
        "        return \"\\n\".join(text_parts)\n",
        "\n",
        "    if suffix in [\".docx\", \".doc\"]:\n",
        "        try:\n",
        "            import docx\n",
        "        except Exception:\n",
        "            raise RuntimeError(\"python-docx is required for DOCX. Install via `pip install python-docx`.\")\n",
        "        doc = docx.Document(str(p))\n",
        "        return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "    raise ValueError(f\"Unsupported file type: {suffix}. Use .txt, .md, .pdf, or .docx\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfYb3bq89iU0"
      },
      "source": [
        "## **Indexing**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1crLYH0y-J99"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Collect your own knowledge base.\n",
        "# 2. Organize it into a .TXT file.\n",
        "# 3. Upload the .TXT file into this Colab.\n",
        "\n",
        "INPUT_FILE = \"./Patch Notes NOV2025_.txt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_vl2e_J_Jhp",
        "outputId": "f253ae78-9c62-49bb-ec13-5c2d213c0ec3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 6132 characters from ./Patch Notes NOV2025_.txt\n"
          ]
        }
      ],
      "source": [
        "# Implement raw_knowledge_base\n",
        "# You should use load_text_from_file function to load text for your knowledge base .TXT file\n",
        "# The variable should be INPUT_FILE\n",
        "raw_knowledge_base = load_text_from_file(INPUT_FILE)\n",
        "print(f\"Loaded {len(raw_knowledge_base)} characters from {INPUT_FILE}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEQcAXcc_Jhp"
      },
      "source": [
        "### **Chunk**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jm2mba8_Jhp",
        "outputId": "22d878ed-0e78-4b43-fbef-1b57482d917c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1.3: The knowledge base was split into 7 smaller Chunks. ðŸ“„ -> ðŸ“„ðŸ“„ðŸ“„\n",
            "Part 2: Expected RAG Answers (Gold Standard)\n",
            "Here is how your Agent should answer the questions based strictly on the text above.\n",
            "\n",
            "1. \"What specific damage adjustments were made to the Reaper Sniper Rifle in the latest update?\"\n",
            "\n",
            "Answer: The patch notes do not state that damage was adjusted. They specify that the Reaper Sniper Rifle received a reduction in bullet speed and an increase in bullet drop because shots were too easy to land.\n",
            "\n",
            "2. \"Has the bug causing players to lose sprint functionality after using FlowBerry Fizz been resolved?\"\n",
            "\n",
            "Answer: Yes, the patch notes list a fix for an issue where players were unable to sprint, which was a known issue often linked to consumable usage.\n",
            "\n",
            "3. \"What are the exact spawn rate changes for the Shield Breaker EMP grenades?\"\n",
            "\n",
            "INSUFFICIENT: I currently do not have specific information regarding \"spawn rate changes\" for the Shield Breaker EMP in my knowledge base. The notes only mention that its damage was increased.\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# convert our string into a LangChain 'Document' object.\n",
        "\n",
        "docs = [Document(page_content=raw_knowledge_base)]\n",
        "\n",
        "# CHUNK_SIZE and CHUNK_OVERLAP .\n",
        "CHUNK_SIZE=1000\n",
        "CHUNK_OVERLAP=100\n",
        "\n",
        "# Implement text_splitter and split_documents\n",
        "# using RecursiveCharacterTextSplitter and text_splitter.split_documents functions\n",
        "# The variables of text_splitter are chunk_size and chunk_overlap\n",
        "# The variable of split_documents is docs\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
        "\n",
        "# splitting our document.\n",
        "split_documents = text_splitter.split_documents(docs)\n",
        "print(f\"Step 1.3: The knowledge base was split into {len(split_documents)} smaller Chunks. ðŸ“„ -> ðŸ“„ðŸ“„ðŸ“„\")\n",
        "\n",
        "#check chunk by commenting out\n",
        "print(split_documents[3].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3u39BZho_Jhp"
      },
      "source": [
        "### **Embed**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KoGoPyIE_Jhp",
        "outputId": "f4b694a2-8e9d-49b0-d5cf-7f1a78d3492b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1.4: Embedding has been created. All text chunks are now represented as numbers. \n"
          ]
        }
      ],
      "source": [
        "# importing the necessary tools.\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Implement embeddings\n",
        "# Using AzureOpenAIEmbeddings function\n",
        "# The variables are azure_endpoint, azure_deployment, api_version, openai_api_key, and default_headers\n",
        "\n",
        "embeddings = AzureOpenAIEmbeddings(\n",
        "    azure_endpoint=AZURE_OPENAI_BASE,\n",
        "    azure_deployment=EMBED_DEPLOYMENT,\n",
        "    api_version=EMBED_API_VERSION,\n",
        "    openai_api_key=AZURE_OPENAI_API_KEY,\n",
        "    default_headers=APIM_HEADERS or None,\n",
        ")\n",
        "\n",
        "print(\"Step 1.4: Embedding has been created. All text chunks are now represented as numbers. \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RuxiFCc_Jhq"
      },
      "source": [
        "### **Store**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pE81qTpq_Jhq",
        "outputId": "73d856b9-bd46-47ed-af67-5e03abb56500"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1.5: Vector store has been created. All text chunks are now stored. \n"
          ]
        }
      ],
      "source": [
        "# Implement vector_store\n",
        "# Using FAISS.from_documents function\n",
        "# The variables are split_documents and embeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "vector_store = FAISS.from_documents(split_documents, embeddings)\n",
        "\n",
        "# Output for chunked text\n",
        "CHUNK_TEXT_OUT = \"nvida_embedded_chunks.txt\"\n",
        "# Save chunked text + vectors:\n",
        "with open(CHUNK_TEXT_OUT, \"w\", encoding=\"utf-8\") as f:\n",
        "    for i, d in enumerate(docs):\n",
        "        f.write(f\"===== CHUNK {i} =====\\n\")\n",
        "        f.write(d.page_content)\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "        # Embed the chunk manually to expose vector\n",
        "        vec = embeddings.embed_query(d.page_content)\n",
        "\n",
        "        # Write vector\n",
        "        f.write(\"VECTOR:\\n\")\n",
        "        f.write(str([vec]))\n",
        "        f.write(\"\\n\\n\")\n",
        "\n",
        "print(\"Step 1.5: Vector store has been created. All text chunks are now stored. \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zppGwvp_Jhq"
      },
      "source": [
        "## **Retrieval**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0m25txpn_Jhq",
        "outputId": "9206e1a9-3587-435a-9dd1-add1c46f524c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 2: The retriever is ready to find relevant information. \n"
          ]
        }
      ],
      "source": [
        "# Implement retriever\n",
        "# using vector_store.as_retriever function\n",
        "# parameter top-K to determine how many revelant chunks are retrieved.\n",
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "print(\"Step 2: The retriever is ready to find relevant information. \")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1T4JG_g_Jhq",
        "outputId": "1ac4c440-223d-45dc-94dc-2ce9a318e221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Retriever Test ---\n",
            "Found 3 relevant documents for the test query.\n",
            "Most relevant document content: \n",
            "...Part 2: Expected RAG Answers (Gold Standard)\n",
            "Here is how your Agent should answer the questions based strictly on the text above.\n",
            "\n",
            "1. \"What specific damage adjustments were made to the Reaper Sniper Rifle in the latest update?\"\n",
            "\n",
            "Answer: The patch notes do not state that damage was adjusted. They specify that the Reaper Sniper Rifle received a reduction in bullet speed and an increase in bullet drop because shots were too easy to land.\n",
            "\n",
            "2. \"Has the bug causing players to lose sprint functionality after using FlowBerry Fizz been resolved?\"\n",
            "\n",
            "Answer: Yes, the patch notes list a fix for an issue where players were unable to sprint, which was a known issue often linked to consumable usage.\n",
            "\n",
            "3. \"What are the exact spawn rate changes for the Shield Breaker EMP grenades?\"\n",
            "\n",
            "INSUFFICIENT: I currently do not have specific information regarding \"spawn rate changes\" for the Shield Breaker EMP in my knowledge base. The notes only mention that its damage was increased....\n",
            "--- End Test ---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# designing a revelant test question to test if your retriever works well.\n",
        "retriever_test_question=\" What are the exact spawn rate changes for the Shield Breaker EMP grenades?\"\n",
        "\n",
        "\n",
        "retrieved_docs = retriever.invoke(retriever_test_question)\n",
        "\n",
        "# Print the retrieval results\n",
        "print(\"\\n--- Retriever Test ---\")\n",
        "print(f\"Found {len(retrieved_docs)} relevant documents for the test query.\")\n",
        "print(f\"Most relevant document content: \\n...{retrieved_docs[0].page_content}...\")\n",
        "print(\"--- End Test ---\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2DXWVY7_Jhq"
      },
      "source": [
        "### **Building the Prompt**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "EECDNH-A_Jhq"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "\n",
        "# ------------ Edit AREA Start---------------------------------\n",
        "# Creating Prompt for Agent Summarization Tasks\n",
        "PROMPT_TEMPLATE = \"\"\"\n",
        "ROLE:\n",
        "You are a technical AI agent specialized in summarizing game patch notes for Fortnite. Your audience includes developers, customer support agents, and players.\n",
        "\n",
        "INSTRUCTION:\n",
        "Use the following retrieved context (raw patch notes) to generate a factual summary of the updates. You must strictly adhere to the context to prevent hallucinations and ensure verifiability. Analyze the text to identify the specific change, the type of change, and the magnitude of the update.\n",
        "\n",
        "CONTEXT:\n",
        "Use the retrieved documents and double-check each chunk to find the correct information regarding game updates, bug fixes, and balance changes.\n",
        "\n",
        "INPUT:\n",
        "{context}\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "Provide a structured response using categorized bullet points labeled with the type of change (e.g., \"Bug Fix,\" \"Balance Update,\" \"New Feature\"). Ensure the summary is concise and organized effectively for quick reading.\n",
        "\n",
        "CONSTRAINTS:\n",
        "- The response must be accurate and concise.\n",
        "- You must maintain high faithfulness; do not invent features or fixes not present in the text.\n",
        "- Maintain a neutral, professional tone suitable for technical documentation.\n",
        "\n",
        "CHECKS:\n",
        "- If the retrieved context does not contain the answer or is unrelated to the query â†’ reply INSUFFICIENT: I currently do not have the specific patch note information in my knowledge base to answer this query.\n",
        "- Verify that every bullet point can be tracked back to a specific data point in the retrieved text.\n",
        "\n",
        "QUESTION:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
        "\n",
        "\n",
        "llm = init_chat_model(\n",
        "    model=CHAT_MODEL_NAME,\n",
        "    model_provider=\"azure_ai\",\n",
        "    endpoint=AZURE_INFERENCE_BASE,\n",
        "    credential=AZURE_INFERENCE_API_KEY,\n",
        "    api_version=CHAT_API_VERSION,\n",
        "    client_kwargs={\"headers\": APIM_HEADERS} if APIM_HEADERS else None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXbpfYhx_Jhq"
      },
      "source": [
        "### **Constructing RAG Pipeline**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wvTwZ_iP_Jhq"
      },
      "outputs": [],
      "source": [
        "#formatting the retrieved documents into a single block of text.\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "\n",
        "# Here's how chain will work:\n",
        "# 1. The user's question comes in.\n",
        "# 2. The `retriever` gets the question and finds the relevant context.\n",
        "# 3. The `prompt` template gets the context and the original question.\n",
        "# 4. The `llm` gets the filled-in prompt and generates the answer.\n",
        "# 5. The `StrOutputParser` cleans up the LLM's output into a simple string.\n",
        "\n",
        "\n",
        "# Implementing rag_chain\n",
        "rag_chain = (\n",
        "    # minor: RunnablePassthrough is an identity block; returns whatever you give it unchanged\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | PROMPT # prompt template\n",
        "    | llm # refer to the llm we will use\n",
        "    | StrOutputParser() # Output format/clean\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_2iLtGn_Jhq"
      },
      "source": [
        "###**Generation**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MA3FRID5_Jhq",
        "outputId": "8e4660b0-149a-4307-d0f0-73a0d9aeb1ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Balance Update: The Reaper Sniper Rifle received a reduction in bullet speed and an increase in bullet drop, as shots were previously too easy to land. (Specific change: decreased bullet speed and increased bullet drop)  \n",
            "- Bug Fix: The issue where players were unable to sprint after using FlowBerry Fizz has been fixed.\n"
          ]
        }
      ],
      "source": [
        "# Designing test questions to test your rag_chain\n",
        "\n",
        "test_question=\"I see there were balance changes to the Reaper Sniper Rifle, but what specifically changed regarding its bullet drop speed, and did they fix the bug where players were unable to sprint after using a FlowBerry Fizz?\"\n",
        "\n",
        "response = rag_chain.invoke(test_question)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q98yvvg__Jhq"
      },
      "source": [
        "## **Evaluate Your RAG system**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgBzjsoJ_Jhq",
        "outputId": "079254a5-4198-4646-ef9b-89937400e41b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[\n",
            "  {\n",
            "    \"question\": \"What specific damage adjustments were made to the Reaper Sniper Rifle in the latest update?\",\n",
            "    \"response\": \"- The patch notes do not specify any damage adjustments to the Reaper Sniper Rifle.\\n- They mention a slight reduction in bullet speed and an increase in bullet drop to address the issue of easy snipes with little counterplay.\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"Has the bug causing players to lose sprint functionality after using FlowBerry Fizz been resolved?\",\n",
            "    \"response\": \"- **Bug Fix:** Yes, the update resolves an issue where players were sometimes unable to sprint, which was often associated with using consumables like FlowBerry Fizz.\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"What are the exact spawn rate changes for the Shield Breaker EMP grenades?\",\n",
            "    \"response\": \"INSUFFICIENT: I currently do not have the specific patch note information in my knowledge base to answer this query.\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"List all weapons that were vaulted in the v28.10 patch notes.\",\n",
            "    \"response\": \"INSUFFICIENT: I currently do not have the specific patch note information in my knowledge base to answer this query.\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"What new movement mechanics were introduced with the Chapter 5 Season 1 launch?\",\n",
            "    \"response\": \"- New movement mechanics introduced with Chapter 5 Season 1 include procedural layering for movement animations.\\n- Initially, movement speeds for crouching and running were slower at launch.\\n- In the v28.10 update, sprint speed and energy regeneration were reverted to match the speeds from Chapter 4, aligning with the current gameplay pace.\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"Did the developers reduce the reload speed of the Frenzy Auto Shotgun?\",\n",
            "    \"response\": \"- No, the developers did not reduce the reload speed of the Frenzy Auto Shotgun.\\n- The patch notes specify that the Fire Rate was reduced from 2.5 to 2.2.\\n- Additionally, the Headshot Multiplier was decreased from x1.75 to x1.65.\\n- There is no mention of any change to the reload speed in the updated patch notes.\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"What specific changes were made to the train's movement speed or route on the map?\",\n",
            "    \"response\": \"INSUFFICIENT: I currently do not have information regarding changes to the train's movement speed or route in the retrieved patch notes.\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"Does the latest patch note mention any changes to the default running speed for player characters?\",\n",
            "    \"response\": \"- Yes, the latest patch notes confirm that sprint speed (and Storm circle speed) were reverted to match the speeds from Chapter 4.\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"What is the new health cap for the Ballistic Shield before it is temporarily disabled?\",\n",
            "    \"response\": \"- INSUFFICIENT: I currently do not have the specific patch note information in my knowledge base to answer this query.\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"Compare the fire rate changes between the Striker AR and the Nemesis AR.\",\n",
            "    \"response\": \"INSUFFICIENT: The retrieved patch notes do not include specific information regarding fire rate changes for the Striker AR or the Nemesis AR.\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"Were there any specific fixes regarding the matchmaking errors in Ranked Zero Build?\",\n",
            "    \"response\": \"- **Bug Fix:** A fix was implemented for an issue that caused incorrect rank progress to display at the end of Ranked Zero Build matches.\"\n",
            "  },\n",
            "  {\n",
            "    \"question\": \"What UI improvements were added to the Locker tab in the most recent patch?\",\n",
            "    \"response\": \"- Added a new \\\"scroll to top\\\" button for both controllers and mouse/keyboard to allow instant navigation to the top of the locker.\\n- Fixed Wrap randomization so that different slots now receive different wraps instead of all slots sharing the same wrap.\\n- Removed the white outline from cosmetic locker icon images for improved visual clarity.\\n- The \\\"Favorites\\\" heart icon now appears in the locker overview for easier management.\"\n",
            "  }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "# Five Evaluation Questions\n",
        "evaluate_questions=[\n",
        "    \"What specific damage adjustments were made to the Reaper Sniper Rifle in the latest update?\", # Balance Update\n",
        "    \"Has the bug causing players to lose sprint functionality after using FlowBerry Fizz been resolved?\", # Bug Fix\n",
        "    \"What are the exact spawn rate changes for the Shield Breaker EMP grenades?\", # Stat Adjustment\n",
        "    \"List all weapons that were vaulted in the v28.10 patch notes.\", # Vaulted/Unvaulted\n",
        "    \"What new movement mechanics were introduced with the Chapter 5 Season 1 launch?\", # New Feature\n",
        "    \"Did the developers reduce the reload speed of the Frenzy Auto Shotgun?\", # Balance Update\n",
        "    \"What specific changes were made to the train's movement speed or route on the map?\", # Map Update\n",
        "    \"Does the latest patch note mention any changes to the default running speed for player characters?\", # Hallucination Check\n",
        "    \"What is the new health cap for the Ballistic Shield before it is temporarily disabled?\", # Stat Adjustment\n",
        "    \"Compare the fire rate changes between the Striker AR and the Nemesis AR.\", # Comparison\n",
        "    \"Were there any specific fixes regarding the matchmaking errors in Ranked Zero Build?\", # Bug Fix\n",
        "    \"What UI improvements were added to the Locker tab in the most recent patch?\" # QoL Update\n",
        "]\n",
        "\n",
        "results=[]\n",
        "\n",
        "# Generation\n",
        "for question in evaluate_questions:\n",
        "  response = rag_chain.invoke(question)\n",
        "  results.append({\n",
        "        \"question\": question,\n",
        "        \"response\": response\n",
        "    })\n",
        "\n",
        "print(json.dumps(results, indent=2, ensure_ascii=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
